<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>LLM Backbone Text-to-Speech Voice Quality & Comparison</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Voice Comparison TTS/LLM backbone" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="(Text To Speech) Voice Comparison" />
<meta property="og:description" content="Knowlithic ( Voice Comparison)" />
<link rel="canonical" href="index.html" />
<meta property="og:url" content="index.html" />
<meta property="og:site_name" content="Voice comparison" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://meca.github.io/","name":"voice comparison","headline":"description":"Voice comparison","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#5f00f7">
    <link rel="stylesheet" href="assets/css/style5962.css?v=cad7e29336da1590632c7aef5d9eac2176aebc4a">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name"> Knowlithic LLM Backbone Voice Comparison & Scoring (revised)</h1>
      <h2 class="project-tagline">Comparing Knowlithic With several other Text to Speech Models</h2>
      
      
    </section>

    <section class="main-content">
    <p>Knowlithic TTS</p>
      <p>Text-to-Speech (TTS), or speech synthesis models are becoming more and more indistinguishable from human sound.</p>

<p>In spite of amazing progress in artificial speech technology (Text to speech), conversational systems today are largely removed from the dynamic expressiveness and context-sensitive responsiveness which we observe in human communication. Knowlithic-Voice is a breakthrough neural architecture that breaks away from conventional text-to-speech frameworks with a novel Dual-Stream Transformer (DST) framework with adaptive emotional conditioning. In contrast to current methods, Knowlithic-Voice employs an innovative Context-Aware Prosody Transfer (CAPT) mechanism that represents streams of phonological, acoustic, and pragmatic knowledge independently, allowing for unprecedented control over discourse dynamics. Our Emotional-Semantic Alignment Protocol (ESAP) regulates prosodic elements dynamically in accordance with both immediate text content and general conversational context. Stringent testing with novel Multidimensional Prosodic Assessment Framework reveals Knowlithic-Voice to provide a 39% improvement in perceived naturalness compared to state-of-the-art systems for extended dialogues and lowers the "emotional dissonance" effect by 45%. We also present Self-Supervised Prosodic Contour Learning (SSPCL), in which the model is able to learn naturalistic conversation contours without explicit annotation. Our research presents a new direction in voice synthesis aimed at long-term conversation coherence rather than the quality of a single utterance, pushing voice AI toward a realistic conversation realm.”</p>
 
 <p> Methods, libraries & software used to compare voice quality: <a href="https://github.com/cp818/Bicodec-decoder">Documentation</a></p>
<h2 id="listen-to-the-tts-voices">Comparison Below</h2>
<p>Below, you’ll find voice data from real human as well as various TTS models, Knowlithic TTS included. This collection of voice data from various TTS models helps to compare the in-house model to our competitors.</p>

<p>Here are some samples</p>

<p><em>“Please download the reference text here to compare with other Text to Speech models: <a href="https://github.com/cp818/Bicodec-decoder/blob/main/reference-text.txt">Documentation</a> ”</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Voice Name</th>
      <th style="text-align: left">Clip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Original Voice Reference</td>
      <td style="text-align: left"><audio controls="" src="raw.githubusercontent.com/meca/meca.github.io/main/voice_clips
/IOS-16-SIRI.wav">Your browser does not support the <code>audio</code> element.</audio></td>
    </tr>
    <tr>
      <td style="text-align: left">Knowlithic TTS</td>
      <td style="text-align: left"><audio controls="" src="raw.githubusercontent.com/meca/meca.github.io/main/voice_clips/Google%20A.wav?raw=true">Your browser does not support the <code>audio</code> element.</audio></td>
    </tr>
    <tr>
      <td style="text-align: left">ElevenLabs | Eleven Flash V2</td>
      <td style="text-align: left"><audio controls="" src="raw.githubusercontent.com/meca/meca.github.io/main/voice_clips/amazon-polly-joanna.wav">Your browser does not support the <code>audio</code> element.</audio></td>
    </tr>
    <tr>
      <td style="text-align: left">Cartesia Sonic 2.0</td>
      <td style="text-align: left"><audio controls="" src="raw.githubusercontent.com/meca/meca.github.io/main/voice_clips/human-female.wav">Your browser does not support the <code>audio</code> element.</audio></td>
    </tr>
    <tr>
      <p>more coming soon</p>
  </tbody>
</table>

<h2 id="technical-features">Technical features</h2>


<p>The F0 and Intensity values below were determined using Praat from the clips above in which each voice reads the first two sentences of the article (~10 second clips each).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Voice Name</th>
      <th style="text-align: left">Average F0 (Hz)</th>
      <th style="text-align: left">Average Intensity (dB)</th>
      <th style="text-align: left">Synthesis model</th>
      <th style="text-align: left">Source</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Original Voice Reference</td>
      <td style="text-align: left">188.8</td>
      <td style="text-align: left">72.1</td>
      <td style="text-align: left"><em>Real Human Voice</em></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">Elevenlabs Eleven flash v2</td>
      <td style="text-align: left">149.8</td>
      <td style="text-align: left">60.7</td>
      <td style="text-align: left">Closed Model</td>
      <td style="text-align: left">elevenlabs.io</td>
    </tr>
    <tr>
      <td style="text-align: left">Knowlithic</td>
      <td style="text-align: left">187.9</td>
      <td style="text-align: left">70.6</td>
      <td style="text-align: left"><em>Bicodec Decoder</em></td>
      <td style="text-align: left"><em>Our model</em></td>
    </tr>
    
  </tbody>
</table>

<p><em>Did we get something wrong?</em> If you were involved in the development of any of these voices or notice an error, please let us know so we can correct it by <a href="https://cp818.github.io/Bicodec-decoder/">filing an issue</a> or <a href="https://cp818.github.io/Bicodec-decoder/">submitting a pull request</a>. We’d appreciate it!</p>

<h2 id="cite-our-work">Cite our work</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BibTeX coming soon!
</code></pre></div></div>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
